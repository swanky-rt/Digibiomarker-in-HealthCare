{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50ab4c5",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.*\n",
    "\n",
    "*Note that this assignment was designed to run in the **Jupyter Notebook** environment. The assignment may not run effectively on Jupyter Lab.*\n",
    "\n",
    "## NOTE\n",
    "To successfully complete this homework, it is highly recommended that you install [Anaconda](https://docs.anaconda.com/anaconda/install/) and create a virtual environment specifically for this assignment. Please ensure that you are using Python version 3.10 or higher.\n",
    "\n",
    "You can create the virtual environment by running the following command:\n",
    "```bash\n",
    "conda create --name cs690r python=3.10 numpy scipy pandas matplotlib jupyter\n",
    "```\n",
    "\n",
    "Once installed, activate the environment before working on the homework:\n",
    "```bash\n",
    "conda activate cs690r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7876ee",
   "metadata": {},
   "source": [
    "The assignment has three tasks:\n",
    "\n",
    "- Task 1: Understanding the limitations of double-integrating acceleration time-series to obtain position time-series and exploring the efficacy of low-pass filter (LPF) in reducing noice.\n",
    "- Task 2: Experimenting with different algorithms to obtain gravity-free acceleartion while the sensor is in motion and assessing the associated challenges to gain insights into the complexity of the task.\n",
    "- Task 3: Experimenting with sensor fusion algorithms to obtain acceleration in the global coordinate system while the sensor is moving randomly."
   ]
  },
  {
   "cell_type": "code",
   "id": "d7a9ac18",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Run some setup code for this notebook\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "from scipy.signal import find_peaks, resample, butter, filtfilt\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# The commands will allow the notebook to reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5077293d",
   "metadata": {},
   "source": [
    "[`Attitude and Heading Reference Systems (AHRS)`](https://ahrs.readthedocs.io/en/latest/index.html) is a Python library for sensor fusion, specifically for estimating orientation from inertial measurement unit (IMU) data, which typically includes acceleration and angular velocity measurements.\n",
    "\n",
    "To download and install the package, you can use the following command in the terminal or command prompt:\n",
    "\n",
    "```bash\n",
    "pip install ahrs\n",
    "```\n",
    "\n",
    "Refer to this [page](https://ahrs.readthedocs.io/en/latest/installation.html) for more detailed information about ahrs installation. \n",
    "\n",
    "Once you install the package, you can import and use the ahrs.filters module for sensor fusion algorithms in you Python scripts or Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "id": "49cfcea4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from ahrs.filters import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26ce7f28",
   "metadata": {},
   "source": [
    "[Rockpool Xylo™](https://rockpool.ai/devices/xylo-overview.html) is a family of ultra-low-power (sub-mW) sensory processing and classification devices. In this homework, we are only going to leverage [Xylo™IMU](https://rockpool.ai/devices/xylo-imu/imu-if.html) for IMU data preprocessing.\n",
    "\n",
    "To download and install the package in the environment you created, you can use the following command in the terminal or command prompt:\n",
    "\n",
    "```bash\n",
    "pip install rockpool\n",
    "```\n",
    "\n",
    "You will also need `samna` and `bitstruct` packages. \n",
    "\n",
    "```bash\n",
    "pip install samna\n",
    "pip install bitstruct\n",
    "```\n",
    "\n",
    "Refer to this [page](https://rockpool.ai/basics/installation.html) for more detailed information about ahrs installation."
   ]
  },
  {
   "cell_type": "code",
   "id": "313d7bd2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from cs690r.data_utils import load_sensor_from_csv, load_mocap_from_tsv, trim_data, ARC\n",
    "from cs690r.plot_utils import plot_time_series, animate_trajectory, compare_trajectory"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64fdce2e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Define gravity constant\n",
    "GRAVITY_CONSTANT = 9.80665"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca93e409",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "You can find the data for Task 1 in the \"data\" folder. In this task, a sensor equipped with accelerometer, gyroscope, and magnetometer was placed on a stationary horizontal surface. To facilitate this process, a function for loading sensor data from text files has been implemented for students. This function is located in the \"cs690r.data_utils\" module.\n",
    "\n",
    "You can access the attributes of sensor data using the following code\n",
    "```python\n",
    "sample_rate = sensor_data.sample_rate # sampling rate of the sensor, 100Hz\n",
    "acc = sensor_data.acc # acceleration, unit: m/s^2\n",
    "gyr = sensor_data.gyr # angular velocity, unit: rad/s\n",
    "mag = sensor_data.mag # magnetometer data, unit: a.u. (arbitrary units; normalized to earth field strength)\n",
    "\n",
    "free_acc = sensor_data.free_acc # the sensor's manufacturer (Movella XSens) has its proprietary algorithm to estimate the gravity-free acceleration in the global coordinates\n",
    "```\n",
    "\n",
    "Please note that the data in the file is raw and unfiltered."
   ]
  },
  {
   "cell_type": "code",
   "id": "27c9ac49",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load stationary data\n",
    "task1_sensor_file = os.path.join('data', 'task1_sensor_data.csv')\n",
    "task1_sensor_data = load_sensor_from_csv(task1_sensor_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49f84e4f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Remove gravity from the z-axis, given that the sensor was placed stationary\n",
    "task1_sensor_data.acc[:, 2] -= GRAVITY_CONSTANT"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41545cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T14:25:11.521446Z",
     "start_time": "2024-01-19T14:25:10.883645Z"
    }
   },
   "source": [
    "## **Task 1.1** \n",
    "\n",
    "1) Implement the `filtering_and_integrate` function provided below. In this function, use a 6th order Butterworth filter with a cut-off frequency of 8 Hz to low-pass filter the raw acceleration and angular velocity, respectively. Integrate the filtered acceleration to derive the velocity time-series, followed by band-pass filtering (2nd order Butterworth) with cut-off frequency between 0.1 Hz and 8 Hz to address integration drift and high-frequency noise. Repeat the integration and band-pass filtering process to the derived velocity time-series to get position time-series.\n",
    "\n",
    "2) Apply `filtering_and_integrate` to `task1_sensor_data`. Specifically, double-integrate the acceleration time-series to obtain position time-series both **with and without filtering**.\n",
    "\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "1) For integration, consider [`scipy.integrate.cumulative_trapezoid`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.cumulative_trapezoid.html) for integration. You are also free to use other integration implementations you are familiar with.\n",
    "\n",
    "2) For filtering, checkout `Filtering Example.ipynb` in this folder to learn how to design different types of filters."
   ]
  },
  {
   "cell_type": "code",
   "id": "3178ba68",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 1.1.1: Implement the `filtering_and_integrate` function provided below\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "def filtering_and_integrate(data, use_filter=True):\n",
    "    '''\n",
    "    Input Parameters\n",
    "    ----------\n",
    "    data : sensor data object\n",
    "        To access the acceleration, use:\n",
    "        acc = data.acc\n",
    "\n",
    "    use_filter : boolean\n",
    "        If use_filter is set to True,\n",
    "        low-pass and band-pass filters will be applied to the data\n",
    "        \n",
    "    Output Parameters\n",
    "    -----------\n",
    "    data : sensor data object with filtered data\n",
    "        data.acc: filtered acceleration\n",
    "        data.vel: integrated, filtered velocity\n",
    "        data.pos: integrated, filtered position\n",
    "        data.gyr: filtered angular velocity\n",
    "    '''\n",
    "    data = copy.deepcopy(data)\n",
    "\n",
    "    # Get acceleration\n",
    "    acc = data.acc.copy()\n",
    "    \n",
    "    # Get angular velocity\n",
    "    gyr = data.gyr.copy() if 'gyr' in dir(data) else None\n",
    "    \n",
    "    # Get the sampling rate of the sensor\n",
    "    sample_rate = data.sample_rate\n",
    "\n",
    "    ####################################################################\n",
    "    # TODO: Define low-pass filter and band-pass filter\n",
    "    # apply the filters on the sensor acceleration and angular velocity\n",
    "    # if use_filter is True\n",
    "    # integrate the acceleration to get velocity and position\n",
    "    def butterworth_filter(x, fs, order, btype, cutoff):\n",
    "        nyq = 0.5 * fs\n",
    "        if btype in [\"low\", \"high\"]:\n",
    "            Wn = cutoff / nyq\n",
    "        elif btype == \"band\":\n",
    "            Wn = [cutoff[0] / nyq, cutoff[1] / nyq]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported btype: {btype}\")\n",
    "\n",
    "        b, a = butter(order, Wn, btype=btype, analog=False)\n",
    "        return filtfilt(b, a, x, axis=0)\n",
    "\n",
    "    lpf_order = 6\n",
    "    lpf_cutoff = 8.0  # Hz\n",
    "\n",
    "    bpf_order = 2\n",
    "    bpf_cutoff = (0.1, 8.0)  # Hz\n",
    "\n",
    "    time = data.time if \"time\" in dir(data) else None\n",
    "\n",
    "    # 1) Low-pass filter raw acc and gyr\n",
    "    if use_filter:\n",
    "        acc = butterworth_filter(acc, sample_rate, lpf_order, \"low\", lpf_cutoff)\n",
    "        if gyr is not None:\n",
    "            gyr = butterworth_filter(gyr, sample_rate, lpf_order, \"low\", lpf_cutoff)\n",
    "\n",
    "    # 2) Integrate acc -> vel\n",
    "    if time is not None:\n",
    "        vel = cumulative_trapezoid(acc, x=time, axis=0, initial=0.0)\n",
    "    else:\n",
    "        vel = cumulative_trapezoid(acc, dx=1.0 / sample_rate, axis=0, initial=0.0)\n",
    "\n",
    "    # 3) Band-pass filter vel (drift + high-frequency noise control)\n",
    "    if use_filter:\n",
    "        vel = butterworth_filter(vel, sample_rate, bpf_order, \"band\", bpf_cutoff)\n",
    "\n",
    "    # 4) Integrate vel -> pos\n",
    "    if time is not None:\n",
    "        pos = cumulative_trapezoid(vel, x=time, axis=0, initial=0.0)\n",
    "    else:\n",
    "        pos = cumulative_trapezoid(vel, dx=1.0 / sample_rate, axis=0, initial=0.0)\n",
    "\n",
    "    # 5) Band-pass filter pos\n",
    "    if use_filter:\n",
    "        pos = butterworth_filter(pos, sample_rate, bpf_order, \"band\", bpf_cutoff)\n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    # Save acceleration, velocity, position, and angular velocity\n",
    "    data.acc = acc\n",
    "    data.vel = vel\n",
    "    data.pos = pos\n",
    "    data.gyr = gyr\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "# Exercise 1.1.2: Apply `filtering_and_integrate` on the task1_sensor_data accelerometer data\n",
    "# to obtain position time-series. Repeat the process with and without filters.\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "####################################################################\n",
    "# TODO: Obtain position time-series with filters.\n",
    "# With filters\n",
    "task1_filtered = filtering_and_integrate(task1_sensor_data, use_filter=True)\n",
    "plot_time_series(task1_filtered.pos, \"pos\", title=\"Task 1 Position (with filtering)\")\n",
    "\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# TODO: Obtain position time-series without filters.\n",
    "\n",
    "# Without filters\n",
    "task1_nofilter = filtering_and_integrate(task1_sensor_data, use_filter=False)\n",
    "plot_time_series(task1_nofilter.pos, \"pos\", title=\"Task 1 Position (no filtering)\")\n",
    "\n",
    "####################################################################"
   ],
   "id": "ed07de66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1.1 Observations\n",
    "\n",
    "Without filtering, the double-integrated position drifts rapidly and grows unbounded over time.\n",
    "This occurs because small accelerometer noise and bias are accumulated twice through numerical integration, producing quadratic drift.\n",
    "\n",
    "With filtering, the 6th-order low-pass filter removes high-frequency noise from acceleration,\n",
    "and the 0.1–8 Hz band-pass filtering after each integration suppresses both low-frequency drift and high-frequency noise.\n",
    "As a result, the estimated position remains smoother and significantly more stable.\n",
    "\n",
    "However, drift is still present, demonstrating that IMU-only position estimation is inherently unstable over long durations."
   ],
   "id": "d78c6a25e4dd7e7"
  },
  {
   "cell_type": "markdown",
   "id": "3d7e6d19",
   "metadata": {},
   "source": [
    "## **Task 1.2**\n",
    "\n",
    "1) Apply `plot_time_series` (located in plot_utils.py) to generate 1D time series plots for acceleration, velocity, and position in each scenario (with and without filtering).\n",
    "\n",
    "2) Animate the position time-series derived from with/without a low-pass filter using the provided `animate_trajectory` (located in plot_utils.py).\n",
    "\n",
    "Even though the sensor was placed on a horizontal surface, the raw unfiltered acceleration does not seem to be perfectly zero due to imperfect calibration and inherent sensor noise. Integrating the raw unfiltered acceleration to obtain velocity and position may cause significant drift over time. However, utilizing a low-pass filter and a band-pass filter can effectively mitigate this drift."
   ]
  },
  {
   "cell_type": "code",
   "id": "a6641f09",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 1.2.1: Plot 1D time series for acceleration, velocity, and position\n",
    "# in each scenario (i.e. with/without filtering) \n",
    "\n",
    "####################################################################\n",
    "# Without filtering\n",
    "task1_nofilter = filtering_and_integrate(task1_sensor_data, use_filter=False)\n",
    "\n",
    "plot_time_series(task1_nofilter.acc, \"acc\", title=\"Task 1 Acceleration (no filtering)\")\n",
    "plot_time_series(task1_nofilter.vel, \"vel\", title=\"Task 1 Velocity (no filtering)\")\n",
    "plot_time_series(task1_nofilter.pos, \"pos\", title=\"Task 1 Position (no filtering)\")\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# With filtering\n",
    "\n",
    "task1_filtered = filtering_and_integrate(task1_sensor_data, use_filter=True)\n",
    "\n",
    "plot_time_series(task1_filtered.acc, \"acc\", title=\"Task 1 Acceleration (with filtering)\")\n",
    "plot_time_series(task1_filtered.vel, \"vel\", title=\"Task 1 Velocity (with filtering)\")\n",
    "plot_time_series(task1_filtered.pos, \"pos\", title=\"Task 1 Position (with filtering)\")\n",
    "\n",
    "\n",
    "####################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1.2.1 Observations\n",
    "\n",
    "In the no-filter case, acceleration contains small bias and noise.\n",
    "After the first integration, velocity shows a clear linear drift.\n",
    "After the second integration, position exhibits strong quadratic drift, growing unbounded over time.\n",
    "\n",
    "With filtering applied, the acceleration signal is smoother.\n",
    "The resulting velocity shows significantly reduced drift, and the position remains bounded with much smaller deviation.\n",
    "This demonstrates how low-pass filtering reduces high-frequency noise,\n",
    "while band-pass filtering mitigates low-frequency drift introduced during integration."
   ],
   "id": "8c5e568832dc13ad"
  },
  {
   "cell_type": "code",
   "id": "57feca80",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 1.2.2: Animate the position time-series \n",
    "# obtained without filtering and with filtering\n",
    "# Use \"animate_trajectory\" from cs690r.data_utils\n",
    "\n",
    "# Set the view limitsfor X, Y, and Z axis for the \n",
    "# animation by passing the correct parameters to \n",
    "# animate_trajectory (Refer to the source code \n",
    "# for details on parameter usage).  \n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "task1_nofilter = filtering_and_integrate(task1_sensor_data, use_filter=False)\n",
    "pos_nf = task1_nofilter.pos\n",
    "\n",
    "xmin, xmax = pos_nf[:,0].min(), pos_nf[:,0].max()\n",
    "ymin, ymax = pos_nf[:,1].min(), pos_nf[:,1].max()\n",
    "zmin, zmax = pos_nf[:,2].min(), pos_nf[:,2].max()\n",
    "\n",
    "anim_nf = animate_trajectory(\n",
    "    pos_nf,\n",
    "    xmin=xmin, xmax=xmax,\n",
    "    ymin=ymin, ymax=ymax,\n",
    "    zmin=zmin, zmax=zmax,\n",
    "    title=\"Task 1: No Filter\"\n",
    ")\n",
    "\n",
    "anim_nf.save(\"task1_no_filter.gif\", writer=\"pillow\", fps=30)\n",
    "display(Image(\"task1_no_filter.gif\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a6f861a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# For the case where filtering is applied,\n",
    "# create two animations:\n",
    "# 1. One animation without specific constraints \n",
    "#    on view limits for each axis (i.e., do not \n",
    "#    pass parameters to animate_trajectory).\n",
    "# 2. The other animation that uses the same view \n",
    "#    limits as the animation without filtering, \n",
    "#    allowing direct comparison.\n",
    "\n",
    "# With filtering\n",
    "task1_filtered = filtering_and_integrate(task1_sensor_data, use_filter=True)\n",
    "pos_f = task1_filtered.pos\n",
    "\n",
    "anim_f_free = animate_trajectory(\n",
    "    pos_f,\n",
    "    title=\"Task 1: Filtered (Free View)\"\n",
    ")\n",
    "\n",
    "anim_f_free.save(\"task1_filter_free.gif\", writer=\"pillow\", fps=30)\n",
    "display(Image(\"task1_filter_free.gif\"))\n",
    "\n",
    "anim_f_fixed = animate_trajectory(\n",
    "    pos_f,\n",
    "    xmin=xmin, xmax=xmax,\n",
    "    ymin=ymin, ymax=ymax,\n",
    "    zmin=zmin, zmax=zmax,\n",
    "    title=\"Task 1: Filtered (Fixed View)\"\n",
    ")\n",
    "\n",
    "anim_f_fixed.save(\"task1_filter_fixed.gif\", writer=\"pillow\", fps=30)\n",
    "display(Image(\"task1_filter_fixed.gif\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1.2.2 Observations\n",
    "\n",
    "In the no-filter case, the trajectory rapidly drifts away from the origin due to accumulated integration errors.\n",
    "The motion appears unbounded, especially in the X and Y directions.\n",
    "\n",
    "In the filtered case without fixed view limits, the trajectory appears stable and tightly bounded near the origin.\n",
    "\n",
    "When using the same view limits as the no-filter case, the filtered trajectory is visibly much smaller in magnitude,\n",
    "highlighting how filtering significantly reduces integration drift.\n",
    "\n",
    "This demonstrates that low-pass filtering removes high-frequency noise,\n",
    "while band-pass filtering mitigates low-frequency bias and drift introduced during double integration."
   ],
   "id": "b59989c9fd219ddb"
  },
  {
   "cell_type": "markdown",
   "id": "ad2af190",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "In this task, an 9-axis inertial measurement unit (referred to as *sensor*) was moved along a rectangular trajectory within 3D space (i.e., the trajectory discussed in class). The sensor captured the three-axis accelerometer, three-axis gyroscope, and three-axis magnetometer time-series. Moreover, a reflective marker was also placed on the sensor to capture the three-axis position time-series using a motion caption system (referred to as *mocap*). \n",
    "\n",
    "Note that we have used *hand clapping* to synchronize the mocap and sensor data.\n",
    "\n",
    "You can find the data for Task 2 in the \"data\" folder. The sensor data were saved in `task_2_sensor_data.csv`, whereas the mocap data were saved in `task2_mocap_data.tsv`. You may use `load_sensor_from_csv` and `load_mocap_from_tsv` functions provided in `data_utils.py` to load the data.\n",
    "\n",
    "You can access the attributes of mocap data by using the following code\n",
    "```python\n",
    "sample_rate = mocap_data.sample_rate # sampling rate of the mocap system, 150Hz\n",
    "raw_pos = mocap_data.raw_pos # position, unit: m\n",
    "```\n",
    "\n",
    "Please note that the data in the files are raw and unfiltered."
   ]
  },
  {
   "cell_type": "code",
   "id": "37156f37",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load sensor data\n",
    "task2_sensor_file = os.path.join('data', 'task2_sensor_data.csv')\n",
    "task2_sensor_data = load_sensor_from_csv(task2_sensor_file)\n",
    "\n",
    "# Load mocap data\n",
    "task2_mocap_file = os.path.join('data', 'task2_mocap_data.tsv')\n",
    "task2_mocap_data = load_mocap_from_tsv(task2_mocap_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a878b424",
   "metadata": {},
   "source": [
    "Run the following code to visualize the raw acceleration and angular velocity captured by the sensor. Note that the data collector clapped multiple times before and after the data collection, resulting in large peaks in the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "87a7eca9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(\"Sensor sample rate:\", task2_sensor_data.sample_rate)\n",
    "print(\"Sensor acc shape:\", task2_sensor_data.acc.shape)\n",
    "print(\"Sensor gyr shape:\", task2_sensor_data.gyr.shape)\n",
    "print(\"task2_mocap_data_keys\", task2_mocap_data.__dict__.keys())\n",
    "\n",
    "print(\"\\nMocap sample rate:\", task2_mocap_data.sample_rate)\n",
    "print(\"Mocap raw position shape:\", task2_mocap_data.pos.shape)\n",
    "\n",
    "# Plot the sensor's raw acceleration and angular velocity\n",
    "n_rows = 2\n",
    "n_cols = 1\n",
    "row_sz = 3\n",
    "col_sz = 7\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(n_cols*col_sz, n_rows*row_sz))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(task2_sensor_data.acc)\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Acc ($m/s^2$)')\n",
    "ax.set_title('Sensor raw acceleration')\n",
    "ax.legend(['X-axis', 'Y-axis', 'Z-axis'])\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(task2_sensor_data.gyr)\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Angular vel ($rad/s$)')\n",
    "ax.set_title('Sensor raw angular velocity')\n",
    "ax.legend(['X-axis', 'Y-axis', 'Z-axis'])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "319d7a9a",
   "metadata": {},
   "source": [
    "## **Task 2.1**\n",
    "\n",
    "1) Implement the `filtering_and_gradient` function provided below. In this function, use a 6th order Butterworth filter with a cut-off frequency of 8 Hz to low-pass filter the raw position captured by the mocap system. Compute the gradient of the filtered position to derive the velocity time-series, followed by the same low-pass filter. Repeat the process of taking gradient and low-pass filtering to the velocity times-series to acquire acceleration time-series.\n",
    "\n",
    "2) Apply `filtering_and_gradient` to `task2_mocap_data`. \n",
    "\n",
    "3) Apply `filtering_and_integrate` from Task 1 to `task2_sensor_data`.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "1) For gradient, you can use [`numpy.gradient`](https://numpy.org/doc/stable/reference/generated/numpy.gradient.html). You are also free to use other integration implementations you are familiar with."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ebdb35d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.1.1 Implement the `filtering_and_gradient` function provided below\n",
    "def filtering_and_gradient(data):\n",
    "    '''\n",
    "    Input Parameters\n",
    "    ----------\n",
    "    data : mocap data object\n",
    "        To access the position, use:\n",
    "        pos = data.pos\n",
    "        \n",
    "    Output Parameters\n",
    "    -----------\n",
    "    data : mocap data object with filtered data\n",
    "        data.pos: filtered position\n",
    "        data.vel: filtered velocity\n",
    "        data.acc: filtered position\n",
    "    '''\n",
    "    data = copy.deepcopy(data)\n",
    "    \n",
    "    # Get raw position\n",
    "    pos = data.pos.copy()\n",
    "    \n",
    "    # Get the sampling rate of the sensor\n",
    "    sample_rate = data.sample_rate\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Define low-pass filter\n",
    "    # apply the filters on the data\n",
    "    # take the gradient of the position to get velocity and acceleration\n",
    "\n",
    "    from scipy import signal\n",
    "    import numpy as np\n",
    "\n",
    "    # 6th order Butterworth low-pass @ 8 Hz\n",
    "    cutoff_hz = 8.0\n",
    "    b_lp, a_lp = signal.butter(\n",
    "        N=6,\n",
    "        Wn=cutoff_hz,\n",
    "        btype=\"low\",\n",
    "        fs=sample_rate\n",
    "    )\n",
    "\n",
    "    # 1) low-pass filter raw position\n",
    "    pos = signal.filtfilt(b_lp, a_lp, pos, axis=0)\n",
    "\n",
    "    # 2) gradient to get velocity, then low-pass\n",
    "    dt = 1.0 / sample_rate\n",
    "    vel = np.gradient(pos, dt, axis=0)\n",
    "    vel = signal.filtfilt(b_lp, a_lp, vel, axis=0)\n",
    "\n",
    "    # 3) gradient to get acceleration, then low-pass\n",
    "    acc = np.gradient(vel, dt, axis=0)\n",
    "    acc = signal.filtfilt(b_lp, a_lp, acc, axis=0)\n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    # Save filtered acceleration, velocity, and position\n",
    "    data.pos = pos\n",
    "    data.vel = vel\n",
    "    data.acc = acc\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82163d56",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.1.2: Apply `filtering_and_gradient` on task2_mocap_data \n",
    "task2_mocap_filtered = filtering_and_gradient(task2_mocap_data)\n",
    "\n",
    "print(\"Filtered mocap pos:\", task2_mocap_filtered.pos.shape)\n",
    "print(\"Filtered mocap vel:\", task2_mocap_filtered.vel.shape)\n",
    "print(\"Filtered mocap acc:\", task2_mocap_filtered.acc.shape)\n",
    "plot_time_series(task2_mocap_filtered.pos, \"pos\", title=\"Task 2 Mocap Position (filtered)\")\n",
    "plot_time_series(task2_mocap_filtered.vel, \"vel\", title=\"Task 2 Mocap Velocity (filtered, gradient)\")\n",
    "plot_time_series(task2_mocap_filtered.acc, \"acc\", title=\"Task 2 Mocap Acceleration (filtered, 2nd gradient)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d2ccfab",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.1.3: Apply `filtering_and_integrate` on task2_sensor_data\n",
    "task2_sensor_filtered = filtering_and_integrate(task2_sensor_data, use_filter=True)\n",
    "\n",
    "print(\"Filtered sensor acc:\", task2_sensor_filtered.acc.shape)\n",
    "print(\"Integrated sensor vel:\", task2_sensor_filtered.vel.shape)\n",
    "print(\"Integrated sensor pos:\", task2_sensor_filtered.pos.shape)\n",
    "\n",
    "plot_time_series(task2_sensor_filtered.acc, \"acc\", title=\"Task 2 Sensor Acceleration (filtered)\")\n",
    "plot_time_series(task2_sensor_filtered.vel, \"vel\", title=\"Task 2 Sensor Velocity (integrated + filtered)\")\n",
    "plot_time_series(task2_sensor_filtered.pos, \"pos\", title=\"Task 2 Sensor Position (double integrated + filtered)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3573cdd",
   "metadata": {},
   "source": [
    "## **Task 2.2**\n",
    "\n",
    "\n",
    "In this task, you will delve into synchronizing data captured the sensor and mocap. Variations in sampling rates or other factors often introduce discrepancies in the collected data. To facilitate synchronization, the data collector performed multiple claps while holding the sensor (which was attached with a mocap marker) before and after data collection. This resulted in large peaks in acceleration, as shown in our previous plots. Leveraging these claps, we can align different data by trimming segments between the claps and resampling to a uniform sampling rate.\n",
    "\n",
    "1) Run the provided code to trim both sensor and mocap data.\n",
    "\n",
    "2) Implement the `resampling_mocap` function. In the function, use [`scipy.signal.resample`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.resample.html) to resample the mocap acceleration, velocity, and position, in order to match the sampling rate of the sensor data. We opt to reduce the sampling rate of the mocap data to align with that of the sensor data, aiming to maintain data integrity (compared to upsampling the sensor data to match the mocap data, which entails data interpolation and may compromise integrity).\n",
    "\n",
    "3) Apply `resampling_mocap` to `task2_mocap_data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cf247fc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Build derived signals first (creates .vel and .pos)\n",
    "task2_sensor_proc = filtering_and_integrate(task2_sensor_data, use_filter=True)\n",
    "\n",
    "# trim_data() also expects free_acc; if you don't have it yet, just set it for now\n",
    "if not hasattr(task2_sensor_proc, \"free_acc\"):\n",
    "    task2_sensor_proc.free_acc = task2_sensor_proc.acc.copy()\n",
    "\n",
    "# Mocap also should be processed first (creates .vel and .acc)\n",
    "task2_mocap_proc = filtering_and_gradient(task2_mocap_data)\n",
    "\n",
    "# Exercise 2.2.1: Trim both sensor and mocap data\n",
    "sensor_start, sensor_end = 1183, 4431  # task 2\n",
    "task2_sensor_proc = trim_data(task2_sensor_proc, sensor_start, sensor_end, \"sensor\")\n",
    "\n",
    "mocap_start, mocap_end = 1981, 6812  # task 2\n",
    "task2_mocap_proc = trim_data(task2_mocap_proc, mocap_start, mocap_end, \"mocap\")\n",
    "print(\"Sensor:\", task2_sensor_proc.acc.shape, task2_sensor_proc.vel.shape, task2_sensor_proc.pos.shape)\n",
    "print(\"Mocap :\", task2_mocap_proc.pos.shape, task2_mocap_proc.vel.shape, task2_mocap_proc.acc.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e773cca3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.2.2: Implement the `resampling_mocap` function provided below\n",
    "def resampling_mocap(data, num):\n",
    "    '''\n",
    "    Input Parameters\n",
    "    ----------\n",
    "    data : mocap data object\n",
    "    \n",
    "    num : int, the number of samples in the resampled signal\n",
    "    \n",
    "    Output Parameters\n",
    "    ----------\n",
    "    data : mocap data object with resampled pos, vel and acc\n",
    "    '''\n",
    "    data = copy.deepcopy(data)\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: resample acceleration, velocity, and position\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    original_len = data.pos.shape[0]\n",
    "\n",
    "    data.pos = resample(data.pos, num, axis=0)\n",
    "    data.vel = resample(data.vel, num, axis=0)\n",
    "    data.acc = resample(data.acc, num, axis=0)\n",
    "\n",
    "    data.sample_rate = data.sample_rate * (num / original_len)\n",
    "    ####################################################################\n",
    "    \n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3356dda6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.2.3: Apply `resampling_mocap` to `task2_mocap_data`.\n",
    "# Number of samples should match trimmed sensor length\n",
    "num_samples = task2_sensor_proc.pos.shape[0]\n",
    "\n",
    "task2_mocap_resampled = resampling_mocap(task2_mocap_proc, num_samples)\n",
    "\n",
    "print(\"Resampled Mocap:\",\n",
    "      task2_mocap_resampled.pos.shape,\n",
    "      task2_mocap_resampled.vel.shape,\n",
    "      task2_mocap_resampled.acc.shape)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "sensor_x = task2_sensor_proc.acc[:,0]\n",
    "mocap_x = task2_mocap_resampled.acc[:,0]\n",
    "\n",
    "sensor_x = (sensor_x - sensor_x.mean()) / sensor_x.std()\n",
    "mocap_x = (mocap_x - mocap_x.mean()) / mocap_x.std()\n",
    "\n",
    "plt.plot(sensor_x, label=\"Sensor X (normalized)\")\n",
    "plt.plot(mocap_x, label=\"Mocap X (normalized)\")\n",
    "plt.legend()\n",
    "plt.title(\"Acceleration Alignment (Normalized)\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86aa0b66",
   "metadata": {},
   "source": [
    "In practical applications, removing gravitational components from local acceleration to obtain gravity-free acceleration is essential. **Task 2.3 – Task 2.7** will explore different approaches to achieve this:\n",
    "1. Applying a highpass filter to remove gravitational components.\n",
    "2. Using Autocorrelation-based Rotation Correction (ARC) algorithm to estimate rotation matrix\n",
    "3. Applying Rodrigues' rotation matrix to the residual component of the highpass filter to estimate local gravity and correct coordinate.\n",
    "4. Fusing local acceleration with angular velocity to estimate sensor orientation in global coordinate system where Z-axis aligns with gravity.\n",
    "5. Using the sensor fusion algorithm provided by the XSens.\n",
    "\n",
    "Finally, in **Task 2.8**, you will compare the performance of each method with the ground truth (i.e., mocap data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa4fac",
   "metadata": {},
   "source": [
    "## **Task 2.3**\n",
    "A high-pass filter removes low-frequency components from local acceleration signals, effectively attenuating gravitational effects. ThisåΩ task will guide you through implementing and applying a band-pass filter, which integrates a high-pass filter to reduce gravitational influence and a low-pass filter to minimize sensor noise, ultimately estimating gravity-free acceleration.\n",
    "\n",
    "1) Implement the `bpf_remove_gravity` function. First, define a $6^{th}$ order Butterworth band-pass filter with cut-off frequency between $[0.1Hz, 20Hz]^{[1]}$. Second, apply the band-pass filter on the local acceleration to obtain gravity-free acceleration. Finally, apply `filtering_and_integrate` from Task 1 to the transformed gravity-free acceleration to derive velocity and position time-series.\n",
    "åçç\n",
    "2) Apply `bpf_remove_gravity` to `task2_sensor_data` to obtain gravity-free acceleration, velocity, and position time-series.  \n",
    "\n",
    "3) Use `compare_trajectory` to visualize and compare the gravity-free acceleration, velocity, and position from  the sensor with the data motion capture (mocap) systems.  \n",
    "\n",
    "**Reference**\n",
    "\n",
    "[1] [Gupta, Anoopum S., et al. \"At-home wearables and machine learning sensitively capture disease progression in amyotrophic lateral sclerosis.\" Nature Communications 14.1 (2023): 5080.](https://www.nature.com/articles/s41467-023-40917-3)"
   ]
  },
  {
   "cell_type": "code",
   "id": "caec373f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.3.1 Implement the `bpf_remove_gravity` function\n",
    "from types import SimpleNamespace\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "def bpf_remove_gravity(local_data):\n",
    "    '''\n",
    "    Input Parameters\n",
    "    ----------\n",
    "    local_data : sensor data object in sensor's coordinates\n",
    "    \n",
    "    Output Parameters\n",
    "    ----------\n",
    "    gravity_free_data : compute gravity-free data\n",
    "        gravity_free_data.sample_rate: the synchornized sampling rate\n",
    "        gravity_free_data.acc: gravity-free 3D accelerometer\n",
    "    '''\n",
    "    \n",
    "    local_data = copy.deepcopy(local_data)\n",
    "    \n",
    "    local_acc = local_data.acc\n",
    "    local_gyr = local_data.gyr\n",
    "    sample_rate = local_data.sample_rate\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Define and apply a Butterworth band-pass filter to obtain\n",
    "    # gravity-free inertial data from local acceleration\n",
    "\n",
    "    order = 6\n",
    "    f_low = 0.1\n",
    "    f_high = 20.0\n",
    "\n",
    "    nyq = 0.5 * sample_rate\n",
    "    low = f_low / nyq\n",
    "    high = f_high / nyq\n",
    "\n",
    "    # safety in case sample_rate is low (rare, but avoids butter() errors)\n",
    "    if high >= 1.0:\n",
    "        high = 0.99\n",
    "    if low <= 0.0:\n",
    "        low = 1e-6\n",
    "\n",
    "    sos = butter(order, [low, high], btype=\"bandpass\", output=\"sos\")\n",
    "    gravity_free_acc = sosfiltfilt(sos, local_acc, axis=0)\n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    # Save the transformed data\n",
    "    gravity_free_data = SimpleNamespace()\n",
    "    gravity_free_data.sample_rate = local_data.sample_rate\n",
    "    gravity_free_data.acc = gravity_free_acc\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Apply filtering_and_integrate to gravity_free_data\n",
    "    gravity_free_data.gyr = local_gyr\n",
    "    gravity_free_data.free_acc = gravity_free_acc.copy()\n",
    "    gravity_free_data = filtering_and_integrate(gravity_free_data, use_filter=False)\n",
    "\n",
    "    ####################################################################\n",
    "\n",
    "    return gravity_free_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8166edfe",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.3.2: Apply `bpf_remove_gravity` on `task2_sensor_data`\n",
    "task2_bpf = bpf_remove_gravity(task2_sensor_proc)\n",
    "print(\"BPF gravity-free acc:\", task2_bpf.acc.shape)\n",
    "print(\"BPF velocity:\", task2_bpf.vel.shape)\n",
    "print(\"BPF position:\", task2_bpf.pos.shape)\n",
    "\n",
    "plot_time_series(task2_bpf.acc, \"acc\", title=\"Task 2 Sensor Acc (BPF gravity-free)\")\n",
    "plot_time_series(task2_bpf.vel, \"vel\", title=\"Task 2 Sensor Vel (BPF integrated)\")\n",
    "plot_time_series(task2_bpf.pos, \"pos\", title=\"Task 2 Sensor Pos (BPF double integrated)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Exercise 2.3.3: Compare acceleration, velocity, \n",
    "# and position from BPF and mocap system\n",
    "\n",
    "compare_trajectory(task2_bpf, task2_mocap_resampled, title1=\"Task 2: BPF gravity-free\", title2=\"Task 2: mocap\")\n"
   ],
   "id": "a0b3d761bf591c7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparison Between BPF-Based Estimation and Mocap Ground Truth\n",
    "\n",
    "To evaluate the effectiveness of the band-pass filtering (BPF) approach, the gravity-free acceleration, velocity, and position obtained from the sensor were compared against the motion capture (mocap) ground truth.\n",
    "\n",
    "**Acceleration Comparison:**\n",
    "The BPF-processed acceleration aligns temporally with the mocap acceleration, indicating successful synchronization. The major motion events occur at similar frames. However, the sensor signal remains noisier and exhibits slight amplitude discrepancies due to residual bias and measurement noise.\n",
    "\n",
    "**Velocity Comparison:**\n",
    "After integrating the gravity-free acceleration, noticeable drift appears in the sensor-derived velocity. In contrast, the mocap velocity remains bounded and stable. This indicates that small residual errors in acceleration accumulate during integration.\n",
    "\n",
    "**Position Comparison:**\n",
    "The divergence becomes more significant at the position level. The sensor-derived position shows substantial drift and unrealistic displacement, while the mocap trajectory remains physically consistent and bounded. This demonstrates how integration amplifies even minor acceleration errors.\n",
    "\n",
    "**Conclusion:**\n",
    "Although band-pass filtering reduces gravitational influence and preserves motion timing, it does not completely eliminate bias. The remaining residual errors accumulate during integration, resulting in significant velocity and position drift. Therefore, band-pass filtering alone is insufficient for robust inertial trajectory estimation when compared to mocap ground truth."
   ],
   "id": "cf2f2873b6d71332"
  },
  {
   "cell_type": "markdown",
   "id": "ee302505",
   "metadata": {},
   "source": [
    "## **Task 2.4**\n",
    "This task explores the Autocorrelation-based Rotation Correction (`ARC`) technique to suppress gravity components using only three-axis acceleroemeter.\n",
    "\n",
    "1) Run the following code and apply `ARC` to `task2_sensor_data` to obtain gravity-free acceleration, velocity and position time-series. You can find the source code of `ARC` in `cs690r/data_utils.py`.\n",
    "\n",
    "2) Use `compare_trajectory` to visualize and compare the gravity-free acceleration, velocity, and position from  the sensor with the data motion capture (mocap) systems. "
   ]
  },
  {
   "cell_type": "code",
   "id": "3a8412f4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.4.1 Run the `ARC` function\n",
    "# from cs690r.data_utils import ARC\n",
    "# print(ARC)\n",
    "# import sys\n",
    "# print(sys.version)\n",
    "# import numpy as np\n",
    "# import rockpool\n",
    "# print(\"numpy:\", np.__version__)\n",
    "# print(\"rockpool:\", rockpool.__version__)\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "sensor_start, sensor_end = 1183, 4431\n",
    "\n",
    "task2_sensor_raw_trim = copy.deepcopy(task2_sensor_data)\n",
    "task2_sensor_raw_trim.acc = task2_sensor_raw_trim.acc[sensor_start:sensor_end]\n",
    "if hasattr(task2_sensor_raw_trim, \"gyr\") and task2_sensor_raw_trim.gyr is not None:\n",
    "    task2_sensor_raw_trim.gyr = task2_sensor_raw_trim.gyr[sensor_start:sensor_end]\n",
    "if hasattr(task2_sensor_raw_trim, \"time\") and task2_sensor_raw_trim.time is not None:\n",
    "    task2_sensor_raw_trim.time = task2_sensor_raw_trim.time[sensor_start:sensor_end]\n",
    "\n",
    "task2_ARC_data = ARC(task2_sensor_raw_trim)\n",
    "task2_ARC_data = filtering_and_integrate(task2_ARC_data)\n",
    "\n",
    "print(\"ARC acc:\", task2_ARC_data.acc.shape)\n",
    "print(\"ARC vel:\", task2_ARC_data.vel.shape)\n",
    "print(\"ARC pos:\", task2_ARC_data.pos.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45ddf030",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.4.2: Compare acceleration, velocity, \n",
    "# and position from ARC and mocap system\n",
    "\n",
    "num_samples_arc = task2_ARC_data.pos.shape[0]\n",
    "task2_mocap_resampled_arc = resampling_mocap(task2_mocap_proc, num_samples_arc)\n",
    "\n",
    "compare_trajectory(\n",
    "    task2_ARC_data,\n",
    "    task2_mocap_resampled_arc,\n",
    "    title1=\"Task 2: ARC gravity-free\",\n",
    "    title2=\"Task 2: mocap\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Task 2.4.2 Comparison (ARC vs Mocap)\n",
    "\n",
    "ARC removes most of the gravity component, and the resulting acceleration falls in a realistic range comparable to mocap. The ARC-derived velocity follows the same overall motion trends as mocap but is smoother and slightly smaller due to filtering and integration effects. The largest mismatch is in position: ARC position shows drift and does not match the clean mocap trajectory because double integration amplifies noise and small biases. Overall, ARC improves acceleration and velocity estimates, while mocap remains the more reliable ground truth for position."
   ],
   "id": "bb762670625f660a"
  },
  {
   "cell_type": "markdown",
   "id": "6393c88c",
   "metadata": {},
   "source": [
    "## **Task 2.5**\n",
    "This task explores a method where local acceleration is estimated using a low-pass filter, and Rodrigues’ rotation matrix is used to transform local acceleration into a global reference frame.\n",
    "\n",
    "1) Implement `rodrigues_gravity_removal`. First, implement the algorithm. You can find a detailed description of the algorithm in the lecture slides. Second, apply the algorithm to the local acceleration to obtain gravity-free acceleration. Finally, apply `filtering_and_integrate` from Task 1 to the gravity-free acceleration to derive velocity and position time-series.\n",
    "\n",
    "2) Apply `rodrigues_gravity_removal` to `task2_sensor_data` to obtain gravity-free acceleration, velocity, and position time-series.\n",
    "\n",
    "3) Utilize `compare_trajectory` to visualize the gravity-free acceleration, velocity, and position from both the sensor and mocap systems.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "1. You can use [`np.linalg.norm`](https://numpy.org/doc/2.2/reference/generated/numpy.linalg.norm.html) to calculate the norm of a vector.\n",
    "\n",
    "2. You can use [`np.cross`](https://numpy.org/doc/stable/reference/generated/numpy.cross.html) to calculate the cross product between two vectors.\n",
    "\n",
    "3. You can use [`np.arccos`](https://numpy.org/doc/2.2/reference/generated/numpy.arccos.html) to calculate trigonometric inverse cosine, element-wise."
   ]
  },
  {
   "cell_type": "code",
   "id": "6b1f916c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.5.1 Implement the `rodrigues_gravity_removal` function\n",
    "import numpy as np\n",
    "m = task2_sensor_data.acc[:300].mean(axis=0)\n",
    "print(m, \"norm:\", np.linalg.norm(m))\n",
    "\n",
    "def rodrigues_gravity_removal(local_data):\n",
    "    '''\n",
    "    Input Parameters\n",
    "    ----------\n",
    "    local_data : sensor data object in sensor's coordinates\n",
    "    \n",
    "    Output Parameters\n",
    "    ----------\n",
    "    gravity_free_data : compute gravity-free data\n",
    "        gravity_free_data.sample_rate: the synchornized sampling rate\n",
    "        gravity_free_data.acc: gravity-free 3D accelerometer\n",
    "    '''\n",
    "    \n",
    "    local_data = copy.deepcopy(local_data)\n",
    "    \n",
    "    local_acc = local_data.acc\n",
    "    local_gyr = local_data.gyr\n",
    "    sample_rate = local_data.sample_rate\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Implement rodrigues_gravity_removal\n",
    "\n",
    "    dt = 1.0 / sample_rate\n",
    "    tau = 2.0\n",
    "    alpha = tau / (tau + dt)\n",
    "\n",
    "    # 1. Estimate gravity direction in local frame using Low-Pass Filter\n",
    "    g_est = np.zeros_like(local_acc)\n",
    "    g_est[0] = local_acc[0]\n",
    "    for i in range(1, len(local_acc)):\n",
    "        g_est[i] = alpha * g_est[i-1] + (1 - alpha) * local_acc[i]\n",
    "\n",
    "    g_ref = np.array([0.0, 0.0, 1.0])  # Target global Z-axis\n",
    "    I = np.eye(3)\n",
    "    eps = 1e-8\n",
    "    gravity_free_acc = np.zeros_like(local_acc)\n",
    "\n",
    "    for i in range(len(local_acc)):\n",
    "        # Normalize local gravity estimate to get direction unit vector\n",
    "        g_vec = g_est[i]\n",
    "        g_mag = np.linalg.norm(g_vec)\n",
    "\n",
    "        if g_mag < eps:\n",
    "            gravity_free_acc[i] = local_acc[i]\n",
    "            continue\n",
    "\n",
    "        a = g_vec / g_mag  # Vector to rotate FROM (local gravity)\n",
    "        b = g_ref          # Vector to rotate TO (global Z)\n",
    "\n",
    "        v = np.cross(a, b)\n",
    "        s = np.linalg.norm(v)\n",
    "        c = np.dot(a, b)\n",
    "\n",
    "        # Calculate Rodrigues Rotation Matrix\n",
    "        if s < eps:\n",
    "            # If vectors are already aligned or opposite\n",
    "            R = I if c > 0 else -I\n",
    "        else:\n",
    "            # Skew-symmetric matrix\n",
    "            K = np.array([\n",
    "                [0,    -v[2],  v[1]],\n",
    "                [v[2],  0,    -v[0]],\n",
    "                [-v[1], v[0],  0]\n",
    "            ])\n",
    "            # R = I + [v]x + [v]x^2 * (1-c)/s^2\n",
    "            R = I + K + (K @ K) * ((1 - c) / (s**2))\n",
    "\n",
    "        # 2. Transform the RAW local acceleration into the Global frame\n",
    "        acc_global = R @ local_acc[i]\n",
    "\n",
    "        # 3. Subtract global gravity (9.81 m/s^2) from the Global Z component\n",
    "        g_mag_actual = np.linalg.norm(g_est[i])\n",
    "        acc_global[2] -= g_mag_actual\n",
    "\n",
    "        gravity_free_acc[i] = acc_global\n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    # Save the transformed data\n",
    "    gravity_free_data = SimpleNamespace()\n",
    "    gravity_free_data.sample_rate = local_data.sample_rate\n",
    "    gravity_free_data.acc = gravity_free_acc\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Apply filtering_and_integrate to gravity_free_data\n",
    "    gravity_free_data = filtering_and_integrate(gravity_free_data)\n",
    "    ####################################################################\n",
    "\n",
    "    return gravity_free_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72acfa19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.5.2: Apply `rodrigues_gravity_removal` on `task2_sensor_data`\n",
    "task2_rodrigues_data = rodrigues_gravity_removal(task2_sensor_data)\n",
    "\n",
    "# Print shapes and stats to verify the transformation\n",
    "print(\"Shapes (acc, vel, pos):\",\n",
    "      task2_rodrigues_data.acc.shape,\n",
    "      task2_rodrigues_data.vel.shape,\n",
    "      task2_rodrigues_data.pos.shape)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d864e594",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.5.3: Compare acceleration, velocity, \n",
    "num_samples = task2_rodrigues_data.acc.shape[0]\n",
    "task2_mocap_resampled = resampling_mocap(task2_mocap_proc, num_samples)\n",
    "\n",
    "start, end = 250, 4600\n",
    "\n",
    "task2_rodrigues_trim = copy.deepcopy(task2_rodrigues_data)\n",
    "task2_rodrigues_trim.acc = task2_rodrigues_data.acc[start:end]\n",
    "task2_rodrigues_trim.vel = task2_rodrigues_data.vel[start:end]\n",
    "task2_rodrigues_trim.pos = task2_rodrigues_data.pos[start:end]\n",
    "\n",
    "task2_mocap_trim = copy.deepcopy(task2_mocap_resampled)\n",
    "task2_mocap_trim.acc = task2_mocap_resampled.acc[start:end]\n",
    "task2_mocap_trim.vel = task2_mocap_resampled.vel[start:end]\n",
    "task2_mocap_trim.pos = task2_mocap_resampled.pos[start:end]\n",
    "compare_trajectory(\n",
    "    task2_rodrigues_trim,\n",
    "    task2_mocap_trim,\n",
    "    title1=\"Task 2: Rodrigues Trimmed\",\n",
    "    title2=\"Task 2: Mocap Trimmed\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Task 2.5.3 – Comparison of Rodrigues Gravity Removal and Mocap\n",
    "\n",
    "After trimming the unstable initial and final segments, the Rodrigues-based gravity removal produces acceleration signals with a magnitude comparable to the mocap ground truth. The overall amplitude range is similar, although the sensor-based acceleration appears noisier due to IMU measurement noise and imperfect gravity estimation.\n",
    "\n",
    "The velocity derived from the sensor follows the general motion trends observed in the mocap data but exhibits more oscillation and slight bias. This behavior is expected since velocity is obtained by integrating noisy acceleration, which amplifies small estimation errors.\n",
    "\n",
    "The position comparison shows noticeable drift and deviation from the clean step-like structure of the mocap trajectory. This is a fundamental limitation of double integration of IMU data, where even small biases accumulate over time.\n",
    "\n",
    "Overall, the Rodrigues gravity removal method effectively suppresses gravity and produces physically reasonable acceleration estimates. However, integration drift and sensor noise result in reduced accuracy compared to mocap measurements, particularly in position estimation."
   ],
   "id": "4f84f3a1ee024d50"
  },
  {
   "cell_type": "markdown",
   "id": "44061069",
   "metadata": {},
   "source": [
    "## **Task 2.6**\n",
    "\n",
    "This task will demonstrate the process of fusing acceleration and angular velocity data collected in the sensor's coordinate system to estimate the sensor's orientation in the global coordinate system. Once the orientation is determined, this information can be utilized to align or rotate the sensor to the global coordinate system.\n",
    "\n",
    "1) Implement the `fuse_and_rotate` function. Firstly, althrough `ahrs` supports a variety of algorithms for estimating the sensor's orientation, we opted to use the AQUA algorithm. Secondly, utilize the estimated orientation to transform the acceleration and angular velocity from the sensor's coordinate system to the global coordinates. Thirdly, remove gravity from the vertical axis of the global coordinate acceleration. Finally, apply `filtering_and_integrate` from Task 1 to the transformed gravity-free, global-coordinate acceleration to derive velocity and position time-series in the global coordinates.\n",
    "\n",
    "2) Apply `fuse_and_rotate` to `task2_sensor_data` to obtain acceleration, velocity, and position time-series in the global coordinates.\n",
    "\n",
    "3) Utilize `compare_trajectory` to visualize the acceleration, velocity, and position from both the sensor and mocap systems in the global coordinates.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "1) You can choose from the following methods provided by AHRS to estimate sensor orientation. The methods shown in the table below assume that the Z-axis of the global coordinate system aligns with gravity. Therefore, you can easily eliminate gravity from the transformed acceleration.\n",
    "\n",
    "| Fusion Method                                                           | Orientatio<br>Representation | What to set inverse in<br>`R.from_quat().apply()` |\n",
    "|-------------------------------------------------------------------------|----------------------------|------------------------------------------------|\n",
    "| [AQUA](https://ahrs.readthedocs.io/en/latest/filters/aqua.html)         | global -> local            | True                                           |\n",
    "| [Madgwich](https://ahrs.readthedocs.io/en/latest/filters/madgwick.html) | local -> global            | False                                          |\n",
    "| [Mahony](https://ahrs.readthedocs.io/en/latest/filters/mahony.html)     | local -> global            | False                                          |\n",
    "\n",
    "Here's an example of sensor orientation estimation using `AQUA` from `ahrs`. Suppose you have acceleration data `acc` in m/s^2 and angular velocity data `gyr` in rad/s, sampled at a frequency of `sample_freq` Hz. To estimate the orientation, you can follow these steps:\n",
    "\n",
    "```python\n",
    "ahrs_filter = AQUA(acc=acc, gyr=gyr, frequency=sample_freq) # define sensor fusion method\n",
    "orientation = ahrs_filter.Q # access the orientation\n",
    "```\n",
    "\n",
    "The `orientation` calculated by `AQUA` represents the global to local transformation, whereas for Madgwick and Mahony, it represents the local to global transformation. You need to be careful with this difference when using them for future coordinate transformation.\n",
    "\n",
    "2) For coordinate transformation, you can use [`scipy.spatial.transform.Rotation`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.html). Assume the local acceleration captured in the sensor local frame is `local_acc` and the `orientation` transforms the data from the sensor local frame to the global frame. To perform transformation,\n",
    "\n",
    "```python\n",
    "orientation = orientation[:, [1, 2, 3, 0]] # the orientation calculated from ahrs has different form than what is required by R.from_quat\n",
    "\n",
    "global_acc = R.from_quat(orientation).apply(local_acc, inverse=True) # inverse needs to be set to `True` because the orientation calcuated from AQUA represents the transformation from global coordinate system to the sensor coordinate system. For Madgwich and Mahony methods, `inverse` needs to be set to `False`.\n",
    "\n",
    "global_acc[:, 2] -= GRAVITY_CONSTANT # remove gravity from Z-axis\n",
    "```\n",
    "\n",
    "In the code above, if `inverse` is specificed `True` in `R.from_quat().apply()`, the inverse of the rotation(s) is applied to the input vectors. You can refer to the table above for guidance on how to set up this parameter."
   ]
  },
  {
   "cell_type": "code",
   "id": "bfb130a3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.6.1 Implement the `fuse_and_rotate` function\n",
    "def fuse_and_rotate(local_data):\n",
    "    '''\n",
    "    Input Parameters\n",
    "    ----------\n",
    "    local_data : sensor data object in sensor's coordinates\n",
    "    \n",
    "    Output Parameters\n",
    "    ----------\n",
    "    global_data : transformed data object in global coordinates\n",
    "        global_data.sample_rate: the synchornized sampling rate\n",
    "        global_data.acc: global 3D accelerometer\n",
    "        global_data.gyr: global 3D gyroscope\n",
    "    '''\n",
    "    \n",
    "    local_data = copy.deepcopy(local_data)\n",
    "    \n",
    "    local_acc = local_data.acc\n",
    "    local_gyr = local_data.gyr\n",
    "    sample_rate = local_data.sample_rate\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Calculate sensor orientation, then use orientation to \n",
    "    # transform acceleration and angular velocity from sensor local \n",
    "    # frame to global frame\n",
    "    # Remove gravity from global acceleration\n",
    "    from ahrs.filters import AQUA\n",
    "    from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "    GRAVITY_CONSTANT = 9.81\n",
    "\n",
    "    # 1) Sensor fusion (AQUA) to estimate orientation (global -> local)\n",
    "    ahrs_filter = AQUA(acc=local_acc, gyr=local_gyr, frequency=sample_rate)\n",
    "    orientation = ahrs_filter.Q  # shape: (N, 4) in [w, x, y, z]\n",
    "\n",
    "    # 2) Reorder quaternion to scipy format: [x, y, z, w]\n",
    "    orientation_xyzw = orientation[:, [1, 2, 3, 0]]\n",
    "\n",
    "    # 3) Rotate local acc and gyr into global frame\n",
    "    # AQUA gives global->local, so to map local->global we apply inverse=True\n",
    "    global_acc = R.from_quat(orientation_xyzw).apply(local_acc, inverse=True)\n",
    "    global_gyr = R.from_quat(orientation_xyzw).apply(local_gyr, inverse=True)\n",
    "\n",
    "    # 4) Remove gravity from global Z axis\n",
    "    global_acc[:, 2] -= GRAVITY_CONSTANT\n",
    "\n",
    "    ####################################################################\n",
    "    \n",
    "    # Save the transformed data\n",
    "    global_data = SimpleNamespace()\n",
    "    global_data.sample_rate = local_data.sample_rate\n",
    "    global_data.acc = global_acc\n",
    "    global_data.gyr = global_gyr\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Apply filtering_and_integrate to global data\n",
    "\n",
    "    global_data = filtering_and_integrate(global_data)\n",
    "    ####################################################################\n",
    "    \n",
    "    return global_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6691bca0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.6.2: Apply `fuse_and_rotate` on `task2_sensor_data`\n",
    "\n",
    "task2_fused_global = fuse_and_rotate(task2_sensor_data)\n",
    "print(task2_fused_global.acc.shape, task2_fused_global.vel.shape, task2_fused_global.pos.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d067a0d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.6.3: Compare acceleration, velocity, and position \n",
    "# from sensor fusion algorithm and mocap system\n",
    "\n",
    "# 1) Resample mocap to match fused sensor length\n",
    "num_samples = task2_fused_global.acc.shape[0]\n",
    "task2_mocap_resampled = resampling_mocap(task2_mocap_proc, num_samples)\n",
    "\n",
    "# 2) Trim both (adjust start/end if needed)\n",
    "start, end = 250, 4600\n",
    "\n",
    "task2_fused_trim = copy.deepcopy(task2_fused_global)\n",
    "task2_fused_trim.acc = task2_fused_global.acc[start:end]\n",
    "task2_fused_trim.vel = task2_fused_global.vel[start:end]\n",
    "task2_fused_trim.pos = task2_fused_global.pos[start:end]\n",
    "\n",
    "task2_mocap_trim = copy.deepcopy(task2_mocap_resampled)\n",
    "task2_mocap_trim.acc = task2_mocap_resampled.acc[start:end]\n",
    "task2_mocap_trim.vel = task2_mocap_resampled.vel[start:end]\n",
    "task2_mocap_trim.pos = task2_mocap_resampled.pos[start:end]\n",
    "\n",
    "# 3) Compare trimmed\n",
    "compare_trajectory(\n",
    "    task2_fused_trim,\n",
    "    task2_mocap_trim,\n",
    "    title1=\"Task 2: Sensor Fusion Trimmed (Global)\",\n",
    "    title2=\"Task 2: Mocap Trimmed (Global)\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2.6 Trimmed Results Analysis\n",
    "\n",
    "After trimming the unstable beginning and ending segments, the sensor fusion results were compared with mocap ground truth.\n",
    "\n",
    "### Acceleration\n",
    "The transformed global acceleration aligns well with mocap data. Motion transitions are clearly captured, and gravity removal on the Z-axis is successful.\n",
    "\n",
    "### Velocity\n",
    "Velocity follows the general motion pattern of the mocap system. However, noticeable drift is present during stationary periods due to small accelerometer bias and integration errors.\n",
    "\n",
    "### Position\n",
    "Position estimation captures overall motion trends but exhibits significant drift and amplitude mismatch. This is expected because position is obtained through double integration of acceleration, which amplifies small bias errors.\n",
    "\n",
    "### Conclusion\n",
    "Sensor fusion improves orientation estimation and global transformation, but pure IMU integration without bias correction leads to velocity and position drift. Additional techniques such as zero-velocity updates or Kalman filtering would be required for accurate long-term position tracking."
   ],
   "id": "20fb1456b6b831a8"
  },
  {
   "cell_type": "markdown",
   "id": "08d25ce7",
   "metadata": {},
   "source": [
    "## **Task 2.7**\n",
    "\n",
    "In this exercise, you will utilize gravity-free acceleration data from the manufacturer's (XSens) proprietary sensor fusion algorithm, which is recognized for its accuracy in integrating multi-modal data to estimate sensor orientation and derive acceleration in global coordinates. This will enable you to evaluate the manufacturer's algorithm performance and compare it with mocap data. Note that the sensor fusion algorithm by Xsens uses all nine-axis inertial data (accelerometer + gyroscope + magnetometer) compared to the AQUA algorithm that uses only six-axis data (accelerometer + gyroscope).\n",
    "\n",
    "1) Apply the `filtering_and_integrate` function to the manufacturer gravity-free acceleration data to derive velocity and position time-series.\n",
    "\n",
    "2) Utilize `compare_trajectory` to visualize the acceleration, velocity, and position from both the manufacturer's gravity-free data and mocap systems in the global coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "id": "fee6ee46",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "task2_xsens_data = SimpleNamespace()\n",
    "task2_xsens_data.sample_rate = task2_sensor_data.sample_rate\n",
    "task2_xsens_data.acc = task2_sensor_data.free_acc\n",
    "task2_xsens_data.gyr = task2_sensor_data.gyr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98bbd198",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.7.1: Apply the `filtering_and_integrate` function \n",
    "# to the xsens gravity-free acceleration data\n",
    "\n",
    "task2_xsens_proc = filtering_and_integrate(task2_xsens_data)\n",
    "\n",
    "print(\"XSens acc:\", task2_xsens_proc.acc.shape)\n",
    "print(\"XSens vel:\", task2_xsens_proc.vel.shape)\n",
    "print(\"XSens pos:\", task2_xsens_proc.pos.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79cc3e6b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.7.2: Compare acceleration, velocity, and position \n",
    "# from xsens and mocap system\n",
    "num_samples = task2_xsens_proc.acc.shape[0]\n",
    "task2_mocap_resampled = resampling_mocap(task2_mocap_proc, num_samples)\n",
    "\n",
    "start, end = 250, 4600\n",
    "\n",
    "task2_xsens_trim = copy.deepcopy(task2_xsens_proc)\n",
    "task2_xsens_trim.acc = task2_xsens_proc.acc[start:end]\n",
    "task2_xsens_trim.vel = task2_xsens_proc.vel[start:end]\n",
    "task2_xsens_trim.pos = task2_xsens_proc.pos[start:end]\n",
    "\n",
    "task2_mocap_trim = copy.deepcopy(task2_mocap_resampled)\n",
    "task2_mocap_trim.acc = task2_mocap_resampled.acc[start:end]\n",
    "task2_mocap_trim.vel = task2_mocap_resampled.vel[start:end]\n",
    "task2_mocap_trim.pos = task2_mocap_resampled.pos[start:end]\n",
    "\n",
    "compare_trajectory(\n",
    "    task2_xsens_trim,\n",
    "    task2_mocap_trim,\n",
    "    title1=\"Task 2: XSens Trimmed (global)\",\n",
    "    title2=\"Task 2: Mocap Trimmed (global)\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2.7 Results Analysis (XSens vs Mocap)\n",
    "\n",
    "After applying filtering and integration to the manufacturer-provided gravity-free acceleration, the XSens results were compared with mocap ground truth.\n",
    "\n",
    "### Acceleration\n",
    "XSens global acceleration aligns closely with mocap data. Motion transitions are well captured and noise levels are low. Gravity removal is accurate.\n",
    "\n",
    "### Velocity\n",
    "Velocity estimation from XSens shows significantly reduced drift compared to the previous methods (Rodrigues and AQUA). Stationary periods remain close to zero, indicating improved bias handling.\n",
    "\n",
    "### Position\n",
    "Position tracking closely follows the mocap trajectory pattern with noticeably reduced drift. Although small integration errors remain, the amplitude and motion timing are much closer to ground truth.\n",
    "\n",
    "### Conclusion\n",
    "The XSens 9-axis fusion algorithm provides substantially better global motion estimation than the 6-axis AQUA and Rodrigues approaches. The inclusion of magnetometer data improves heading stability and reduces integration drift, resulting in more accurate velocity and position estimation."
   ],
   "id": "936e54b20e832221"
  },
  {
   "cell_type": "markdown",
   "id": "2ab9c7ee",
   "metadata": {},
   "source": [
    "## **Task 2.8**\n",
    "\n",
    "In this exercise, you will evaluate the performance of the results from **Task 2.3 - 2.7** both quantitatively and qualitatively.\n",
    "\n",
    "1) Define the function that calculate the root mean square error (RMSE) and the normalized root mean square error (NRMSE) between two given time-series. NRMSE is obtained by normalizing the RMSE to the range of ground truth, so that the error can be represented in percentage (%). \n",
    "\n",
    "2) Apply your function and compute RMSE and NRMSE between the gravity-free acceleration, velocity, and position time-series obtained from **Task 2.3 - 2.7** with the mocap data, respectively. The ground truth is mocap data.\n",
    "\n",
    "This analysis will provide insights into the accuracy and reliability of your sensor fusion algorithm compared to the ground truth mocap data.\n",
    "\n",
    "3) Create animations using `animate_trajectory` for the three position time-series derived from **Task 2.3 - 2.7** as well as the one captured by the mocap system, respectively. By visually comparing these animations, you can qualitatively assess how well the sensor fusion algorithm aligns with the mocap system in representing the sensor's movements."
   ]
  },
  {
   "cell_type": "code",
   "id": "9b96a76a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.1: Define the function for calculating\n",
    "# RMSE and NRMSE between two given time series\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rmse_nrmse(pred, gt, eps=1e-12):\n",
    "    \"\"\"\n",
    "    pred, gt: arrays shaped (T, 3) for x/y/z\n",
    "    returns:\n",
    "      rmse_axis: (3,)\n",
    "      nrmse_axis_pct: (3,) in %\n",
    "      rmse_all: scalar (over all axes)\n",
    "      nrmse_all_pct: scalar in %\n",
    "    \"\"\"\n",
    "    pred = np.asarray(pred)\n",
    "    gt = np.asarray(gt)\n",
    "    assert pred.shape == gt.shape, f\"Shape mismatch: {pred.shape} vs {gt.shape}\"\n",
    "\n",
    "    err = pred - gt\n",
    "    rmse_axis = np.sqrt(np.mean(err**2, axis=0))  # per axis\n",
    "\n",
    "    # normalize by ground-truth range per axis\n",
    "    gt_range_axis = (gt.max(axis=0) - gt.min(axis=0))\n",
    "    nrmse_axis_pct = (rmse_axis / (gt_range_axis + eps)) * 100.0\n",
    "\n",
    "    # overall (all axes together)\n",
    "    rmse_all = np.sqrt(np.mean(err**2))\n",
    "    gt_range_all = (gt.max() - gt.min())\n",
    "    nrmse_all_pct = (rmse_all / (gt_range_all + eps)) * 100.0\n",
    "\n",
    "    return rmse_axis, nrmse_axis_pct, rmse_all, nrmse_all_pct\n",
    "\n",
    "\n",
    "def print_metrics(tag, pred_obj, gt_obj):\n",
    "    for name in [\"acc\", \"vel\", \"pos\"]:\n",
    "        pred = getattr(pred_obj, name)\n",
    "        gt = getattr(gt_obj, name)\n",
    "        rmse_axis, nrmse_axis_pct, rmse_all, nrmse_all_pct = rmse_nrmse(pred, gt)\n",
    "        print(f\"\\n{tag} | {name.upper()}\")\n",
    "        print(f\"  RMSE axis [x y z]: {rmse_axis}\")\n",
    "        print(f\"  NRMSE axis % [x y z]: {nrmse_axis_pct}\")\n",
    "        print(f\"  RMSE all: {rmse_all:.6f}\")\n",
    "        print(f\"  NRMSE all %: {nrmse_all_pct:.3f}\")\n",
    "\n",
    "import copy\n",
    "\n",
    "def resample_and_trim(sensor_obj, mocap_proc, start=None, end=None):\n",
    "    # 1) resample mocap to sensor length\n",
    "    n = sensor_obj.acc.shape[0]\n",
    "    mocap_rs = resampling_mocap(mocap_proc, n)\n",
    "\n",
    "    # 2) optional trimming\n",
    "    if start is None or end is None:\n",
    "        return sensor_obj, mocap_rs\n",
    "\n",
    "    sensor_trim = copy.deepcopy(sensor_obj)\n",
    "    mocap_trim = copy.deepcopy(mocap_rs)\n",
    "\n",
    "    for f in [\"acc\", \"vel\", \"pos\"]:\n",
    "        setattr(sensor_trim, f, getattr(sensor_obj, f)[start:end])\n",
    "        setattr(mocap_trim, f, getattr(mocap_rs, f)[start:end])\n",
    "\n",
    "    return sensor_trim, mocap_trim\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "239e00b0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.2.1: Use your function to caculate RMSE and NRMSE\n",
    "# between the acceleration, velocity, and position from Task 2.3\n",
    "# and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "\n",
    "start, end = 250, 4600  # keep consistent with your earlier comparisons\n",
    "\n",
    "task23_s, mocap23 = resample_and_trim(task2_bpf, task2_mocap_proc, start, end)\n",
    "print_metrics(\"Task 2.3 (BPF)\", task23_s, mocap23)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caca06dc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.2.2: Use your function to caculate RMSE and NRMSE\n",
    "# between the acceleration, velocity, and position from Task 2.4\n",
    "# and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "\n",
    "task24_s, mocap24 = resample_and_trim(task2_ARC_data, task2_mocap_proc, start, end)\n",
    "print_metrics(\"Task 2.4 (ARC)\", task24_s, mocap24)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01686cd0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.2.3: Use your function to caculate RMSE and NRMSE\n",
    "# between the acceleration, velocity, and position from Task 2.5\n",
    "# and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "task25_s, mocap25 = resample_and_trim(task2_rodrigues_data, task2_mocap_proc, start, end)\n",
    "print_metrics(\"Task 2.5 (Rodrigues)\", task25_s, mocap25)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e27a533",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.2.4: Use your function to caculate RMSE and NRMSE\n",
    "# between the acceleration, velocity, and position from Task 2.6\n",
    "# and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "task26_s, mocap26 = resample_and_trim(task2_fused_global, task2_mocap_proc, start, end)\n",
    "print_metrics(\"Task 2.6 (AQUA Fusion Global)\", task26_s, mocap26)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "926d4ee8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.2.5: Use your function to caculate RMSE and NRMSE\n",
    "# between the acceleration, velocity, and position from Task 2.7\n",
    "# and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "\n",
    "task2_xsens_proc = filtering_and_integrate(task2_xsens_data)\n",
    "task27_s, mocap27 = resample_and_trim(task2_xsens_proc, task2_mocap_proc, start, end)\n",
    "print_metrics(\"Task 2.7 (XSens Global)\", task27_s, mocap27)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3f91e845d94cd1ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41297a65",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.3.1: Animate the position time-series\n",
    "# captured by the motion capture system\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "start, end = 250, 4600\n",
    "fps = 30\n",
    "out_gif = \"task2_mocap.gif\"\n",
    "\n",
    "mocap_pos = np.asarray(task2_mocap_proc.pos)[start:end]\n",
    "ani = animate_trajectory(mocap_pos, title=\"2.8.3.1: Mocap Position\")\n",
    "ani.save(out_gif, writer=\"pillow\", fps=fps)\n",
    "display(Image(filename=out_gif))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fa9fa76",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.3.2: Animate the position time-series\n",
    "# derived from Task 2.3\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "start, end = 250, 4600\n",
    "fps = 30\n",
    "out_gif = \"task23_bpf.gif\"\n",
    "\n",
    "ani = animate_trajectory(task23_s.pos, title=\"2.8.3.2: Task 2.3 (BPF)\")\n",
    "ani.save(out_gif, writer=\"pillow\", fps=30)\n",
    "\n",
    "display(Image(filename=out_gif))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d3adb27",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.3.3: Animate the position time-series \n",
    "# derived from Task 2.4\n",
    "\n",
    "out_gif = \"task24_arc.gif\"\n",
    "\n",
    "ani = animate_trajectory(task24_s.pos, title=\"2.8.3.3: Task 2.4 (ARC)\")\n",
    "ani.save(out_gif, writer=\"pillow\", fps=30)\n",
    "\n",
    "display(Image(filename=out_gif))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7946db0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.3.4: Animate the position time-series \n",
    "# derived from Task 2.5\n",
    "\n",
    "out_gif = \"task25_rodrigues.gif\"\n",
    "\n",
    "ani = animate_trajectory(task25_s.pos, title=\"2.8.3.4: Task 2.5 (Rodrigues)\")\n",
    "ani.save(out_gif, writer=\"pillow\", fps=30)\n",
    "\n",
    "display(Image(filename=out_gif))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc5c29ff",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.3.5: Animate the position time-series \n",
    "# derived from Task 2.6\n",
    "out_gif = \"task26_aqua.gif\"\n",
    "\n",
    "ani = animate_trajectory(task26_s.pos, title=\"2.8.3.5: Task 2.6 (AQUA Fusion)\")\n",
    "ani.save(out_gif, writer=\"pillow\", fps=30)\n",
    "\n",
    "display(Image(filename=out_gif))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c154168",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 2.8.3.6: Animate the position time-series \n",
    "# derived from Task 2.7\n",
    "out_gif = \"task27_xsens.gif\"\n",
    "\n",
    "ani = animate_trajectory(task27_s.pos, title=\"2.8.3.6: Task 2.7 (XSens)\")\n",
    "ani.save(out_gif, writer=\"pillow\", fps=30)\n",
    "\n",
    "display(Image(filename=out_gif))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8279ff9",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "You can find the data for Task 3 in the \"data\" folder. In this task, a nine-axis IMU and a mocap marker was placed on a human subject's wrist, who performed some arbitrary, patternless, and random upper-limb movements (e.g., swinging the arm in the air randomly). The sensor data were saved in `task_3_sensor_data.csv`, and the mocap data were saved in `task3_mocap_data.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "id": "1defc57b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load sensor data\n",
    "task3_sensor_file = os.path.join('data', 'task3_sensor_data.csv')\n",
    "task3_sensor_data = load_sensor_from_csv(task3_sensor_file)\n",
    "\n",
    "# Load mocap data\n",
    "task3_mocap_file = os.path.join('data', 'task3_mocap_data.tsv')\n",
    "task3_mocap_data = load_mocap_from_tsv(task3_mocap_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f693398a",
   "metadata": {},
   "source": [
    "The objective of Task 3 is to apply the abovementioned procedures (e.g., **Task 2.1 - 2.8**) to this dataset, aiming to assess the algorithm's capability to handle entirely unpredictable movements that simulate daily scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa07ec9",
   "metadata": {},
   "source": [
    "## **Task 3.1**\n",
    "Apply `filtering_and_gradient` to `task3_mocap_data`. "
   ]
  },
  {
   "cell_type": "code",
   "id": "c5583d64",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.1: Apply `filtering_and_gradient` to `task3_mocap_data`\n",
    "task3_mocap_proc = filtering_and_gradient(task3_mocap_data)\n",
    "\n",
    "print(\"Task3 mocap acc:\", task3_mocap_proc.acc.shape)\n",
    "print(\"Task3 mocap vel:\", task3_mocap_proc.vel.shape)\n",
    "print(\"Task3 mocap pos:\", task3_mocap_proc.pos.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81a9d494",
   "metadata": {},
   "source": [
    "## **Task 3.2**\n",
    "Apply `filtering_and_integrate` to `task3_sensor_data` with filtering. "
   ]
  },
  {
   "cell_type": "code",
   "id": "8069ee44",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.2: Apply `filtering_and_integrate` to `task3_sensor_data` with filtering. \n",
    "task3_sensor_proc = filtering_and_integrate(task3_sensor_data, use_filter=True)\n",
    "\n",
    "print(\"Task3 sensor acc:\", task3_sensor_proc.acc.shape)\n",
    "print(\"Task3 sensor vel:\", task3_sensor_proc.vel.shape)\n",
    "print(\"Task3 sensor pos:\", task3_sensor_proc.pos.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e930f2f",
   "metadata": {},
   "source": [
    "## **Task 3.3**\n",
    "Synchronization and resampling. Trim the sensor data and mocap between the provided clapping indices. Then, apply `resampling_mocap` to resample to the mocap data to match the sampling rate of sensor data."
   ]
  },
  {
   "cell_type": "code",
   "id": "d79e4ee0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.3: Synchronization and resampling\n",
    "task3_sensor_proc = filtering_and_integrate(task3_sensor_data, use_filter=True)\n",
    "task3_mocap_proc  = filtering_and_gradient(task3_mocap_data)\n",
    "sensor_start, sensor_end = 918, 5198\n",
    "task3_sensor_proc = trim_data(task3_sensor_proc, sensor_start, sensor_end, 'sensor')\n",
    "\n",
    "mocap_start, mocap_end = 1300, 7719\n",
    "task3_mocap_proc = trim_data(task3_mocap_proc, mocap_start, mocap_end, 'mocap')\n",
    "\n",
    "####################################################################\n",
    "# TODO: Apply `resampling_mocap` to resample to the mocap data\n",
    "n = task3_sensor_proc.acc.shape[0]\n",
    "task3_mocap_resampled = resampling_mocap(task3_mocap_proc, n)\n",
    "\n",
    "print(\"Sensor pos:\", task3_sensor_proc.pos.shape)\n",
    "print(\"Resampled mocap pos:\", task3_mocap_resampled.pos.shape)\n",
    "####################################################################\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b328836a",
   "metadata": {},
   "source": [
    "## **Task 3.4**\n",
    "1) Repeat Task 2.3 and apply `bpf_remove_gravity` on the sensor data.\n",
    "\n",
    "2) Repeat Task 2.4 and apply `ARC` on the sensor data.\n",
    "\n",
    "3) Repeat Task 2.5 and apply `rodrigues_gravity_removal` on the sensor data.\n",
    "\n",
    "4) Repeat Task 2.6 and apply `fuse_and_rotate` on the sensor data.\n",
    "\n",
    "5) Repeat Task 2.7 and obtain the data provided by the xsens."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca3adaf2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.4.1\n",
    "####################################################################\n",
    "# TODO: Apply `bpf_remove_gravity` on the sensor data.\n",
    "task3_bpf = bpf_remove_gravity(task3_sensor_data)\n",
    "#task3_bpf = filtering_and_integrate(task3_bpf)\n",
    "\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# TODO: Compare acceleration, velocity, and position \n",
    "# from Exercise 3.4.1 and mocap system\n",
    "n = task3_bpf.acc.shape[0]\n",
    "task3_mocap_bpf = resampling_mocap(task3_mocap_proc, n)\n",
    "%matplotlib inline\n",
    "\n",
    "compare_trajectory(\n",
    "    task3_bpf,\n",
    "    task3_mocap_bpf,\n",
    "    title1=\"Task 3: BPF gravity-free\",\n",
    "    title2=\"Task 3: mocap\"\n",
    ")\n",
    "\n",
    "####################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a38534ce",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.4.2\n",
    "####################################################################\n",
    "# TODO: Run `xyloIMU` on the sensor data.\n",
    "import copy\n",
    "\n",
    "# 1) Run xyloIMU if it exists, else use ARC\n",
    "if \"xyloIMU\" in globals():\n",
    "    task3_xylo = xyloIMU(copy.deepcopy(task3_sensor_data))\n",
    "else:\n",
    "    task3_xylo = ARC(copy.deepcopy(task3_sensor_data))\n",
    "\n",
    "# 2) Integrate to get vel/pos\n",
    "task3_xylo = filtering_and_integrate(task3_xylo)\n",
    "\n",
    "# 3) Resample mocap to match\n",
    "n = task3_xylo.acc.shape[0]\n",
    "task3_mocap_xylo = resampling_mocap(task3_mocap_proc, n)\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# TODO: Compare acceleration, velocity, and position \n",
    "# from Exercise 3.4.2 and mocap system\n",
    "compare_trajectory(\n",
    "    task3_xylo,\n",
    "    task3_mocap_xylo,\n",
    "    title1=\"Task 3: xyloIMU / ARC gravity-free\",\n",
    "    title2=\"Task 3: mocap\"\n",
    ")\n",
    "\n",
    "####################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "703b2ae6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.4.3\n",
    "####################################################################\n",
    "# TODO: Apply `rodrigues_gravity_removal` on the sensor data.\n",
    "task3_rodrigues = rodrigues_gravity_removal(task3_sensor_data)\n",
    "\n",
    "n = task3_rodrigues.acc.shape[0]\n",
    "task3_mocap_rodrigues = resampling_mocap(task3_mocap_proc, n)\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# TODO: Compare acceleration, velocity, and position \n",
    "# from Exercise 3.4.3 and mocap system\n",
    "compare_trajectory(\n",
    "    task3_rodrigues,\n",
    "    task3_mocap_rodrigues,\n",
    "    title1=\"Task 3: Rodrigues gravity-free\",\n",
    "    title2=\"Task 3: mocap\"\n",
    ")\n",
    "####################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79e0531d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.4.4\n",
    "####################################################################\n",
    "# TODO: Apply `fuse_and_rotate` on the sensor data.\n",
    "import copy\n",
    "\n",
    "task3_fused = fuse_and_rotate(copy.deepcopy(task3_sensor_data))\n",
    "\n",
    "n = task3_fused.acc.shape[0]\n",
    "task3_mocap_fused = resampling_mocap(task3_mocap_proc, n)\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# TODO: Compare acceleration, velocity, and position \n",
    "# from Exercise 3.4.4 and mocap system\n",
    "compare_trajectory(\n",
    "    task3_fused,\n",
    "    task3_mocap_fused,\n",
    "    title1=\"Task 3: AQUA Fusion (Global)\",\n",
    "    title2=\"Task 3: mocap\"\n",
    ")\n",
    "####################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "906740e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "task3_xsens_data = SimpleNamespace()\n",
    "task3_xsens_data.sample_rate = task3_sensor_data.sample_rate\n",
    "task3_xsens_data.acc = task3_sensor_data.free_acc\n",
    "task3_xsens_data.gyr = task3_sensor_data.gyr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe001944",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.4.5\n",
    "####################################################################\n",
    "# TODO: Apply `filter_and_integrate` \n",
    "# on task3_xsens_data\n",
    "task3_xsens_proc = filtering_and_integrate(task3_xsens_data)\n",
    "\n",
    "n = task3_xsens_proc.acc.shape[0]\n",
    "task3_mocap_xsens = resampling_mocap(task3_mocap_proc, n)\n",
    "####################################################################\n",
    "\n",
    "####################################################################\n",
    "# TODO: Compare acceleration, velocity, and position \n",
    "# from Exercise 3.4.5 and mocap system\n",
    "compare_trajectory(\n",
    "    task3_xsens_proc,\n",
    "    task3_mocap_xsens,\n",
    "    title1=\"Task 3: XSens (Global)\",\n",
    "    title2=\"Task 3: mocap\"\n",
    ")\n",
    "####################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b42a0bd",
   "metadata": {},
   "source": [
    "## **Task 3.5**\n",
    "Repeat **Task 2.6** on `task3_sensor_data` and \n",
    "\n",
    "1) Calculate the RMSE and NRMSE between the acceleration, velocity, and position time-series derived from **Task 3.4** and the mocap system, respectively.\n",
    "\n",
    "2) Create animations using `animate_trajectory` for the position time-series derived from **Task 3.4** and captured by the mocap system, respectively."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6089fd1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.1.1 Calculate RMSE and NRMSE between \n",
    "# the acceleration, velocity, and position from \n",
    "# bpf_remove_gravity and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def rmse_nrmse(pred, gt, eps=1e-12):\n",
    "    pred = np.asarray(pred)\n",
    "    gt = np.asarray(gt)\n",
    "    assert pred.shape == gt.shape, f\"Shape mismatch: {pred.shape} vs {gt.shape}\"\n",
    "\n",
    "    err = pred - gt\n",
    "    rmse_axis = np.sqrt(np.mean(err**2, axis=0))\n",
    "\n",
    "    gt_range_axis = gt.max(axis=0) - gt.min(axis=0)\n",
    "    nrmse_axis_pct = (rmse_axis / (gt_range_axis + eps)) * 100.0\n",
    "\n",
    "    rmse_all = np.sqrt(np.mean(err**2))\n",
    "    gt_range_all = gt.max() - gt.min()\n",
    "    nrmse_all_pct = (rmse_all / (gt_range_all + eps)) * 100.0\n",
    "\n",
    "    return rmse_axis, nrmse_axis_pct, rmse_all, nrmse_all_pct\n",
    "\n",
    "\n",
    "def print_metrics(tag, pred_obj, gt_obj):\n",
    "    for name in [\"acc\", \"vel\", \"pos\"]:\n",
    "        pred = getattr(pred_obj, name)\n",
    "        gt = getattr(gt_obj, name)\n",
    "\n",
    "        rmse_axis, nrmse_axis_pct, rmse_all, nrmse_all_pct = rmse_nrmse(pred, gt)\n",
    "\n",
    "        print(f\"\\n{tag} | {name.upper()}\")\n",
    "        print(f\"  RMSE axis [x y z]: {rmse_axis}\")\n",
    "        print(f\"  NRMSE axis % [x y z]: {nrmse_axis_pct}\")\n",
    "        print(f\"  RMSE all: {rmse_all:.6f}\")\n",
    "        print(f\"  NRMSE all %: {nrmse_all_pct:.3f}\")\n",
    "\n",
    "\n",
    "def resample_mocap_to_method(mocap_proc, method_obj):\n",
    "    n = method_obj.acc.shape[0]\n",
    "    return resampling_mocap(mocap_proc, n)\n",
    "\n",
    "task3_mocap_bpf_m = resample_mocap_to_method(task3_mocap_proc, task3_bpf)\n",
    "print_metrics(\"Task 3.4.1 (BPF)\", task3_bpf, task3_mocap_bpf_m)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "312d1779",
   "metadata": {
    "scrolled": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Exercise 3.5.1.2 Calculate RMSE and NRMSE between \n",
    "# the acceleration, velocity, and position from \n",
    "# xyloIMU and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "task3_mocap_xylo_m = resample_mocap_to_method(task3_mocap_proc, task3_xylo)\n",
    "print_metrics(\"Task 3.4.2 (xyloIMU/ARC)\", task3_xylo, task3_mocap_xylo_m)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7c6b0d8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.1.3 Calculate RMSE and NRMSE between \n",
    "# the acceleration, velocity, and position from \n",
    "# rodrigues_gravity_removal and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "task3_mocap_rod_m = resample_mocap_to_method(task3_mocap_proc, task3_rodrigues)\n",
    "print_metrics(\"Task 3.4.3 (Rodrigues)\", task3_rodrigues, task3_mocap_rod_m)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f984acdb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.1.4 Calculate RMSE and NRMSE between \n",
    "# the acceleration, velocity, and position from \n",
    "# fuse_and_rotate and mocap data\n",
    "\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "task3_mocap_fused_m = resample_mocap_to_method(task3_mocap_proc, task3_fused)\n",
    "print_metrics(\"Task 3.4.4 (AQUA Fusion)\", task3_fused, task3_mocap_fused_m)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d094de2f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.1.5 Calculate RMSE and NRMSE between \n",
    "# the acceleration, velocity, and position from \n",
    "# xsens-provided gravity free data and mocap data\n",
    "\n",
    "# Print RMSE and NRMSE values\n",
    "task3_mocap_xsens_m = resample_mocap_to_method(task3_mocap_proc, task3_xsens_proc)\n",
    "print_metrics(\"Task 3.4.5 (XSens)\", task3_xsens_proc, task3_mocap_xsens_m)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ef14a86",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.2.1: Animate the position time-series \n",
    "# captured by the mocap system\n",
    "%matplotlib widget\n",
    "ani_mocap = animate_trajectory(\n",
    "    task3_mocap_proc.pos,\n",
    "    title=\"Task 3: Mocap Position\"\n",
    ")\n",
    "ani_mocap\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d0a3041",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.2.2: Animate the position time-series \n",
    "# derived from the bpf_remove_gravity\n",
    "ani_bpf = animate_trajectory(\n",
    "    task3_bpf.pos,\n",
    "    title=\"Task 3.4.1: BPF Position\"\n",
    ")\n",
    "ani_bpf\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "854b31b6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.2.3: Animate the position time-series \n",
    "# derived from the ARC\n",
    "ani_xylo = animate_trajectory(\n",
    "    task3_xylo.pos,\n",
    "    title=\"Task 3.4.2: xyloIMU / ARC Position\"\n",
    ")\n",
    "ani_xylo\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba383104",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.2.4: Animate the position time-series \n",
    "# derived from the rodrigues_gravity_removal\n",
    "ani_rodrigues = animate_trajectory(\n",
    "    task3_rodrigues.pos,\n",
    "    title=\"Task 3.4.3: Rodrigues Position\"\n",
    ")\n",
    "ani_rodrigues\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a31d8bfb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.2.5: Animate the position time-series \n",
    "# derived from the fuse_and_rotate\n",
    "ani_aqua = animate_trajectory(\n",
    "    task3_fused.pos,\n",
    "    title=\"Task 3.4.4: AQUA Fusion Position\"\n",
    ")\n",
    "ani_aqua\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3986027d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exercise 3.5.2.6: Animate the position time-series \n",
    "# derived from the xsens data\n",
    "ani_xsens = animate_trajectory(\n",
    "    task3_xsens_proc.pos,\n",
    "    title=\"Task 3.4.5: XSens Position\"\n",
    ")\n",
    "ani_xsens\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ad332c7",
   "metadata": {
    "scrolled": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs690r (py311)",
   "language": "python",
   "name": "cs690r-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
